{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This script processes YouTube video URLs from files in a specified directory and extracts video IDs.\n",
    "Modules:\n",
    "    os: Provides a way of using operating system dependent functionality like reading or writing to the file system.\n",
    "    re: Provides regular expression matching operations.\n",
    "    urllib.parse: Used for parsing URLs.\n",
    "Variables:\n",
    "    src_dir (str): The directory containing the files to be processed.\n",
    "    videos_url (list): A list to store the processed video URLs.\n",
    "    videos_id (list): A list to store the extracted video IDs.\n",
    "Functions:\n",
    "    None\n",
    "Processing:\n",
    "    1. Loop through all files in the specified directory.\n",
    "    2. Open each file and read it line by line.\n",
    "    3. Clean each line by removing specific unwanted characters.\n",
    "    4. Use regular expressions to remove time parameters from the URLs.\n",
    "    5. Append the cleaned URL to the videos_url list.\n",
    "    6. Parse the URL to extract the video ID and append it to the videos_id list.\n",
    "\"\"\"\n",
    "\n",
    "src_dir = 'import'\n",
    "videos_url = []\n",
    "videos_id = []\n",
    "\n",
    "# loop through all files in the import directory\n",
    "for file in os.listdir(src_dir):\n",
    "    with open(os.path.join(src_dir, file)) as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            line = line.replace(' \\u200b', '')\n",
    "            url = re.sub(r'[&?]t=\\d+[hms]?(&?)', r'\\1', line)\n",
    "            videos_url.append(url)\n",
    "\n",
    "            parsed_url = urlparse(url)\n",
    "            if parsed_url.query:\n",
    "                id = parse_qs(parsed_url.query).get(['v'][0])\n",
    "            \n",
    "            id = url.split('=')[1]\n",
    "            videos_id.append(id)\n",
    "\n",
    "#print(videos_url)\n",
    "#print(videos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in videos_id:\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(v)\n",
    "        formatter = TextFormatter()\n",
    "        formatted_transcript = formatter.format_transcript(transcript, indent=2)\n",
    "        with open(f'export/{v}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(formatted_transcript)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing video {v}: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='w'\n",
      "page_content='e'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    #length_function=len,\n",
    "    #is_separator_regex=False\n",
    ")\n",
    "\n",
    "with open('export/-Xj7zDwwU_I.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "texts = splitter.create_documents(data)\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(videos_data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert to Python dict instead of JSON string\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m split_transcript \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_text(json_data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:131\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_text\u001b[0;34m(self, json_data, convert_lists, ensure_ascii)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     json_data: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    127\u001b[0m     convert_lists: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m     ensure_ascii: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Splits JSON into a list of JSON formatted strings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_json(json_data\u001b[38;5;241m=\u001b[39mjson_data, convert_lists\u001b[38;5;241m=\u001b[39mconvert_lists)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Convert to string\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mdumps(chunk, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:117\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_json\u001b[0;34m(self, json_data, convert_lists)\u001b[0m\n\u001b[1;32m    115\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_to_dict_preprocessing(json_data))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(json_data)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Remove the last chunk if it's empty\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:105\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._json_split\u001b[0;34m(self, data, current_path, chunks)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(value, new_path, chunks)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# handle single item\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_nested_dict(chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], current_path, data)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:61\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._set_nested_dict\u001b[0;34m(d, path, value)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     60\u001b[0m     d \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39msetdefault(key, {})\n\u001b[0;32m---> 61\u001b[0m d[path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "src_dir = 'export'\n",
    "videos_data = []\n",
    "\n",
    "for file in os.listdir(src_dir):\n",
    "    f = os.path.join(src_dir, file)\n",
    "    videos_data.append(f)\n",
    "\n",
    "data = json.load(open(videos_data[0], 'r'))\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "splitter = RecursiveJsonSplitter(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=100  # Optional but recommended for better chunk control\n",
    ")\n",
    "\n",
    "data = json.load(open(videos_data[0], 'r'))\n",
    "\n",
    "# Convert to Python dict instead of JSON string\n",
    "split_transcript = splitter.split_text(json_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(videos_data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert to Python dict instead of JSON string\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m split_transcript \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_text(json_data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:131\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_text\u001b[0;34m(self, json_data, convert_lists, ensure_ascii)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     json_data: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    127\u001b[0m     convert_lists: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m     ensure_ascii: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Splits JSON into a list of JSON formatted strings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_json(json_data\u001b[38;5;241m=\u001b[39mjson_data, convert_lists\u001b[38;5;241m=\u001b[39mconvert_lists)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Convert to string\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mdumps(chunk, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:117\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_json\u001b[0;34m(self, json_data, convert_lists)\u001b[0m\n\u001b[1;32m    115\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_to_dict_preprocessing(json_data))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(json_data)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Remove the last chunk if it's empty\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:105\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._json_split\u001b[0;34m(self, data, current_path, chunks)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(value, new_path, chunks)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# handle single item\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_nested_dict(chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], current_path, data)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:61\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._set_nested_dict\u001b[0;34m(d, path, value)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     60\u001b[0m     d \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39msetdefault(key, {})\n\u001b[0;32m---> 61\u001b[0m d[path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# Initialize splitter with appropriate chunk size\n",
    "splitter = RecursiveJsonSplitter(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=100  # Optional but recommended for better chunk control\n",
    ")\n",
    "\n",
    "# Example data structure\n",
    "\n",
    "data = json.load(open(videos_data[0], 'r'))\n",
    "\n",
    "# Convert to Python dict instead of JSON string\n",
    "split_transcript = splitter.split_text(json_data=data)\n",
    "\n",
    "# For MongoDB results (list of dicts), use create_documents instead:\n",
    "# chunks = splitter.create_documents(texts=mongo_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"section1\": {\"content\": \"Your JSON data here...\", \"subsections\": [\"item1\", \"item2\", \"item3\"]}}\n"
     ]
    }
   ],
   "source": [
    "print(split_transcript[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
