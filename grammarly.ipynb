{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This script processes YouTube video URLs from files in a specified directory and extracts video IDs.\n",
    "Modules:\n",
    "    os: Provides a way of using operating system dependent functionality like reading or writing to the file system.\n",
    "    re: Provides regular expression matching operations.\n",
    "    urllib.parse: Used for parsing URLs.\n",
    "Variables:\n",
    "    src_dir (str): The directory containing the files to be processed.\n",
    "    videos_url (list): A list to store the processed video URLs.\n",
    "    videos_id (list): A list to store the extracted video IDs.\n",
    "Functions:\n",
    "    None\n",
    "Processing:\n",
    "    1. Loop through all files in the specified directory.\n",
    "    2. Open each file and read it line by line.\n",
    "    3. Clean each line by removing specific unwanted characters.\n",
    "    4. Use regular expressions to remove time parameters from the URLs.\n",
    "    5. Append the cleaned URL to the videos_url list.\n",
    "    6. Parse the URL to extract the video ID and append it to the videos_id list.\n",
    "\"\"\"\n",
    "\n",
    "src_dir = 'import'\n",
    "videos_url = []\n",
    "videos_id = []\n",
    "\n",
    "# loop through all files in the import directory\n",
    "for file in os.listdir(src_dir):\n",
    "    with open(os.path.join(src_dir, file)) as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            line = line.replace(' \\u200b', '')\n",
    "            url = re.sub(r'[&?]t=\\d+[hms]?(&?)', r'\\1', line)\n",
    "            videos_url.append(url)\n",
    "\n",
    "            parsed_url = urlparse(url)\n",
    "            if parsed_url.query:\n",
    "                id = parse_qs(parsed_url.query).get(['v'][0])\n",
    "            \n",
    "            id = url.split('=')[1]\n",
    "            videos_id.append(id)\n",
    "\n",
    "#print(videos_url)\n",
    "#print(videos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in videos_id:\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(v)\n",
    "        formatter = TextFormatter()\n",
    "        formatted_transcript = formatter.format_transcript(transcript, indent=2)\n",
    "        with open(f'export/{v}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(formatted_transcript)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing video {v}: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome to class number one session\n",
      "number\n",
      "two a nano structure is an object that\n",
      "have at least one dimension in the range\n",
      "of 1 to 100\n",
      "nanometers in describing Nano structures\n",
      "it is needed to differentiate between\n",
      "the number of dimensions on the on the\n",
      "nanoscale nanoclusters are structures\n",
      "that are 1 to 100 nanometer in each spal\n",
      "Dimension these structures are\n",
      "categorized as Zero Dimensional Nano\n",
      "structures nanot tubes and nanowires\n",
      "have a characteristic diameter between 1\n",
      "and 100 nanom and a length that could be\n",
      "much greater than\n",
      "that these structures are categorized as\n",
      "one-dimensional Nano\n",
      "structures nanocomposite surfaces or\n",
      "thin films have a thickness between 1\n",
      "and 100 nanomer while the other two\n",
      "dimensions are much greater these\n",
      "structures are categorized as two\n",
      "dimensional complex\n",
      "materials finally bulk materials with\n",
      "all Dimensions above 100 nanometer but\n",
      "that contain zero Dimension or one\n",
      "dimension and or two dimensional Nano\n",
      "structures are termed threedimensional\n",
      "Nano structures we will describe each of\n",
      "these Nano structure in more details in\n",
      "the next\n",
      "sessions we will start now with defining\n",
      "the Zero Dimensional Nano structure\n",
      "which include basically the nanop\n",
      "particles and the quantum\n",
      "dots nanoparticles are defined as small\n",
      "objects that are sized between 1 and 100\n",
      "nanometers and that behave as a whole\n",
      "unit with respect to its transport\n",
      "properties nanoparticles are size\n",
      "dependent namely the properties of the\n",
      "materials change as their size size\n",
      "approaches the nanoscale and as the\n",
      "percentage of the atoms at the surface\n",
      "of the material become\n",
      "significant the interesting and the\n",
      "unexpected properties of the nanop\n",
      "particles are therefore significantly\n",
      "due to the large surface area of the\n",
      "material which dominates the\n",
      "contributions by made by the small bulk\n",
      "of the\n",
      "material for the sake of comparison bulk\n",
      "materials\n",
      "mainly particles larger than one\n",
      "micrometer contain insignificant\n",
      "percentage of atoms at the surface in\n",
      "relation to the number of atoms in the\n",
      "bulk of the same\n",
      "material and therefore they don't behave\n",
      "or exhibit size dependent changes in\n",
      "their physical\n",
      "properties nanoparticles often posess\n",
      "unexpected Optical properties as they\n",
      "are small enough to confine their\n",
      "electrons and produce Quantum\n",
      "effects the size dependent color of the\n",
      "nanop particles was utilized though\n",
      "without any intention by artists as far\n",
      "as the 9th century for generating a\n",
      "glittering effects on the surface of\n",
      "pots or colors in stained\n",
      "glass the unique physical properties of\n",
      "the nanoparticles allow much higher\n",
      "absorption of solar radiation in\n",
      "photovolatic cells that are composed of\n",
      "nanop particles than in thin films of\n",
      "continuous sheets even that is composed\n",
      "from the same\n",
      "material other size dependent properties\n",
      "change include Quantum confinement in\n",
      "semiconductor particles surface plasma\n",
      "resonance in some metal nanop particles\n",
      "and chemical reactivity that are\n",
      "utilized for image formation and\n",
      "photography\n",
      "field now\n",
      "from the Zero Dimensional Nano\n",
      "structures we will move to\n",
      "onedimensional structures which include\n",
      "among the rest Nano wires Quantum wires\n",
      "nanor roads and nanot\n",
      "tubes in nanowire is a nano structure\n",
      "with the diameter of the order of\n",
      "nanometer alternatively nanowires can be\n",
      "defined as structures that are having a\n",
      "thickness or a diameter constraint to\n",
      "tens of nanometers or less and an\n",
      "unconstrained\n",
      "length at this scales quantum mechanical\n",
      "effects are\n",
      "important and therefore they are coined\n",
      "the term quantum\n",
      "wires many different types of Nano wires\n",
      "exist these include of course metallic\n",
      "nanowires semiconducting nanowires and\n",
      "insulating nanowires more details about\n",
      "if each of these Nano wires could be\n",
      "seen on the slide and we will explain\n",
      "about these in the next\n",
      "lectures on the other hand molecular\n",
      "Nano wires are composed of repeating\n",
      "molecular units either orgenic for\n",
      "example DNA or in orgenic\n",
      "material new forms of Nano wires include\n",
      "Corell superlattices\n",
      "nanowires as seen in the bottom figure\n",
      "in the\n",
      "slide nanowires have two Quantum\n",
      "confined directions while still leaving\n",
      "One unconfined Direction for electrical\n",
      "conduction basically this feature allows\n",
      "the\n",
      "nanowire to be used in applications\n",
      "where electrical conduction is\n",
      "required and because of their unique\n",
      "density of electron states nanowires in\n",
      "the limit of small diameters are\n",
      "expected to exhibit significantly\n",
      "different Optical electrical and\n",
      "magnetic properties from their bulk\n",
      "threedimensional\n",
      "crystalline\n",
      "counterparts we will move right now to\n",
      "the carbon non\n",
      "tubes caronan tubes are long Hol\n",
      "structure with the walls formed by one\n",
      "atom thick sheet of carbon which we call\n",
      "usually in the scientific literature\n",
      "graphine these sheets are rolled at\n",
      "specific and discrete chyal\n",
      "angles and the combination of the\n",
      "Rolling angle and the radius decides the\n",
      "nanot tube\n",
      "properties individual nanot tubes\n",
      "naturally align themselves into ropes\n",
      "held together by the so-called Vander\n",
      "vales forces or more specifically pie\n",
      "stacking usually the end of the carbon N\n",
      "Tube end with half bucky ball like\n",
      "carbon\n",
      "structure carbon n tubes have any usual\n",
      "properties which are valuable for the\n",
      "nanotechnology Electronics Optics and\n",
      "other fields of Materials Science and\n",
      "Technology in particular on to their\n",
      "extraordinary thermal conductivity and\n",
      "mechanical and electrical properties\n",
      "carbon tubes find applications as\n",
      "additives to various Structural\n",
      "Materials for instance n tubes form a\n",
      "tiny portion of the material in some of\n",
      "their carbon fiber baseball pads golf\n",
      "clubs or car\n",
      "parts an in organic nanotube is often\n",
      "composed of metal\n",
      "oxides in organic nanot tubes show\n",
      "various advantages such as easy\n",
      "synthetic AIS and high\n",
      "Christianity good uniformity and\n",
      "dispersion pretty defined electrical\n",
      "conductivity good adhesion to a number\n",
      "of polymers and high impact\n",
      "resistance these materials are therefore\n",
      "prisic candidates as filters for polymer\n",
      "Composites with with enhanced thermal\n",
      "mechanical and electrical\n",
      "properties in organic nanot tubes are\n",
      "heavier than the carbon an tube which we\n",
      "have explained in the last slide and\n",
      "therefore they\n",
      "don't have a\n",
      "strongness under tensil stress but they\n",
      "are particularly strong under\n",
      "compression thus leading to potential\n",
      "applications in Impact resistant\n",
      "applications such as bull roof\n",
      "visits two representative examples of\n",
      "inorganic nanot tubes include as could\n",
      "be seen on the slide boron nitride nanot\n",
      "tubes this case for example or this\n",
      "materal material have a high resistance\n",
      "to oxidation suited for high\n",
      "temperature it has also young modulus of\n",
      "1.22 terapascal it's also behaved as a\n",
      "semiconducting material and of course\n",
      "the electronic properties of this\n",
      "material are\n",
      "predictable and on the other hand I\n",
      "would represent on the same slide uh an\n",
      "example on the Silicon carbon an tubes\n",
      "which have also High resistance to\n",
      "oxidation and it's quite suitable for\n",
      "Harsh environments and can be\n",
      "functionalized with Organic mons on\n",
      "their\n",
      "surfaces with this we come now to the\n",
      "end of class number one session number\n",
      "two thank\n",
      "you\n",
      "page_content='w'\n",
      "page_content='e'\n",
      "page_content='l'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "with open('export/-Xj7zDwwU_I.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "texts = splitter.create_documents(data)\n",
    "print(data)\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(videos_data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert to Python dict instead of JSON string\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m split_transcript \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_text(json_data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:131\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_text\u001b[0;34m(self, json_data, convert_lists, ensure_ascii)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     json_data: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    127\u001b[0m     convert_lists: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m     ensure_ascii: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Splits JSON into a list of JSON formatted strings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_json(json_data\u001b[38;5;241m=\u001b[39mjson_data, convert_lists\u001b[38;5;241m=\u001b[39mconvert_lists)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Convert to string\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mdumps(chunk, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:117\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_json\u001b[0;34m(self, json_data, convert_lists)\u001b[0m\n\u001b[1;32m    115\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_to_dict_preprocessing(json_data))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(json_data)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Remove the last chunk if it's empty\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:105\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._json_split\u001b[0;34m(self, data, current_path, chunks)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(value, new_path, chunks)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# handle single item\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_nested_dict(chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], current_path, data)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:61\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._set_nested_dict\u001b[0;34m(d, path, value)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     60\u001b[0m     d \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39msetdefault(key, {})\n\u001b[0;32m---> 61\u001b[0m d[path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "src_dir = 'export'\n",
    "videos_data = []\n",
    "\n",
    "for file in os.listdir(src_dir):\n",
    "    f = os.path.join(src_dir, file)\n",
    "    videos_data.append(f)\n",
    "\n",
    "data = json.load(open(videos_data[0], 'r'))\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "splitter = RecursiveJsonSplitter(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=100  # Optional but recommended for better chunk control\n",
    ")\n",
    "\n",
    "data = json.load(open(videos_data[0], 'r'))\n",
    "\n",
    "# Convert to Python dict instead of JSON string\n",
    "split_transcript = splitter.split_text(json_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(videos_data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert to Python dict instead of JSON string\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m split_transcript \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_text(json_data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:131\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_text\u001b[0;34m(self, json_data, convert_lists, ensure_ascii)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     json_data: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    127\u001b[0m     convert_lists: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m     ensure_ascii: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Splits JSON into a list of JSON formatted strings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_json(json_data\u001b[38;5;241m=\u001b[39mjson_data, convert_lists\u001b[38;5;241m=\u001b[39mconvert_lists)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Convert to string\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mdumps(chunk, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:117\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter.split_json\u001b[0;34m(self, json_data, convert_lists)\u001b[0m\n\u001b[1;32m    115\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_to_dict_preprocessing(json_data))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(json_data)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Remove the last chunk if it's empty\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:105\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._json_split\u001b[0;34m(self, data, current_path, chunks)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_split(value, new_path, chunks)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# handle single item\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_nested_dict(chunks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], current_path, data)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt311/lib/python3.11/site-packages/langchain_text_splitters/json.py:61\u001b[0m, in \u001b[0;36mRecursiveJsonSplitter._set_nested_dict\u001b[0;34m(d, path, value)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     60\u001b[0m     d \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39msetdefault(key, {})\n\u001b[0;32m---> 61\u001b[0m d[path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# Initialize splitter with appropriate chunk size\n",
    "splitter = RecursiveJsonSplitter(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=100  # Optional but recommended for better chunk control\n",
    ")\n",
    "\n",
    "# Example data structure\n",
    "\n",
    "data = json.load(open(videos_data[0], 'r'))\n",
    "\n",
    "# Convert to Python dict instead of JSON string\n",
    "split_transcript = splitter.split_text(json_data=data)\n",
    "\n",
    "# For MongoDB results (list of dicts), use create_documents instead:\n",
    "# chunks = splitter.create_documents(texts=mongo_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"section1\": {\"content\": \"Your JSON data here...\", \"subsections\": [\"item1\", \"item2\", \"item3\"]}}\n"
     ]
    }
   ],
   "source": [
    "print(split_transcript[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
